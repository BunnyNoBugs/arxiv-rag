{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1glxObYWwfr",
        "outputId": "94c082ce-9ee9-4acc-83a0-44e25c0e04e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: langchain_mistralai in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.12)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.25)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (2.10.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain_mistralai) (0.20.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.12->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain_mistralai) (2.27.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.26.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (4.66.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community arxiv pymupdf langchain_mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_community.retrievers import ArxivRetriever\n",
        "from langchain_core.messages import AIMessage, HumanMessage"
      ],
      "metadata": {
        "id": "z45IM_ZMZt7O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass()\n",
        "os.environ[\"MISTRAL_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig1DirXGbBku",
        "outputId": "3a1875db-92a3-4b82-fa0c-ad6ce24be20c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No query generator"
      ],
      "metadata": {
        "id": "D-DxxIylypP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "\n",
        "# Prompt template with added history\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant who answers scientific questions using data from an articles' database.\n",
        "    This data will be given to you each time, and it is called context.\n",
        "    Answer the user's question based only on this context provided.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Conversation history (include recent exchanges):\n",
        "{history}\n",
        "\n",
        "User's current question: {question}\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain logic with message history handling\n",
        "def build_chain(retriever):\n",
        "    return (\n",
        "        {\n",
        "            # Extract only \"question\" for the retriever\n",
        "            \"context\": {\"question\": RunnablePassthrough() | (lambda x: x[\"question\"]) | retriever | format_docs},\n",
        "            \"question\": RunnablePassthrough(),  # Pass question directly\n",
        "            \"history\": RunnablePassthrough(),   # Pass conversation history\n",
        "        }\n",
        "        | prompt  # Pass all inputs into the prompt\n",
        "        | llm     # Pass the prompt result into the LLM\n",
        "        | StrOutputParser()  # Parse LLM output\n",
        "    )\n",
        "\n",
        "# Example interaction\n",
        "def handle_user_input(question: str, retriever: BaseRetriever):\n",
        "    global history\n",
        "    chain = build_chain(retriever)\n",
        "\n",
        "    # Append the current question to the history\n",
        "    user_input = f\"User: {question}\"\n",
        "    formatted_history = \"\\n\".join(history)\n",
        "    response = chain.invoke({\"question\": question, \"history\": formatted_history})\n",
        "\n",
        "    # Append both user input and response to history\n",
        "    assistant_response = f\"Assistant: {response}\"\n",
        "    history.append(user_input)\n",
        "    history.append(assistant_response)\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example use\n",
        "retriever = ArxivRetriever(\n",
        "    top_k_results=5,\n",
        "    get_full_documents=True,\n",
        "    doc_content_chars_max=10000000000\n",
        ")\n",
        "response = handle_user_input(\n",
        "    \"How does ImageBind model bind multiple modalities into a single embedding space? Tell me in detail\",\n",
        "    retriever\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wikvkLB6yc24",
        "outputId": "1c9154af-7307-478d-8739-0e57a061a86d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ImageBind model binds multiple modalities into a single embedding space by leveraging a shared representation space across six different modalities: image/video, text, audio, depth, thermal images, and IMU data. This is achieved through a process called contrastive learning, where the model learns to align the embeddings from different modalities to a common space.\n",
            "\n",
            "The key steps in this process are:\n",
            "\n",
            "1. **Image Encoding**: The model uses a Vision Transformer (ViT) to encode images into embeddings.\n",
            "2. **Text Encoding**: Text is encoded using a text encoder that maps textual descriptions to embeddings.\n",
            "3. **Audio Encoding**: Audio is converted into mel-spectrograms and then encoded using a Vision Transformer, treating the spectrogram as a 2D signal.\n",
            "4. **Depth and Thermal Encoding**: Depth and thermal images are treated as single-channel images and encoded similarly to RGB images.\n",
            "5. **IMU Encoding**: IMU data is encoded using a simple feedforward network.\n",
            "6. **Contrastive Learning**: The model is trained to minimize the distance between embeddings of paired data from different modalities while maximizing the distance between embeddings of unrelated data.\n",
            "\n",
            "By aligning these embeddings into a common space, the model can perform tasks such as cross-modal retrieval and zero-shot classification, where it can recognize and categorize data from modalities it wasn't explicitly trained on.\n",
            "\n",
            "This binding process allows the model to understand and generate responses for inputs from various modalities, enabling multi-modal instruction following.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = handle_user_input(\n",
        "    \"Nice, can you tell me more about the third step? And please also write source article of your answer this time.\",\n",
        "    retriever\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY0L_wZHFg1-",
        "outputId": "c8c852b7-1bf1-4214-9b9a-bea53148c02b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The third step in the process of binding multiple modalities into a single embedding space involves contrastive learning. This step is crucial for aligning the embeddings from different modalities into a common space. Here's how it works:\n",
            "\n",
            "1. **Contrastive Learning**: The model is trained to minimize the distance between embeddings of paired data from different modalities while maximizing the distance between embeddings of unrelated data. This is achieved using a contrastive loss function, which encourages the embeddings of semantically similar data from different modalities to be close to each other in the embedding space, while pushing the embeddings of dissimilar data apart.\n",
            "\n",
            "2. **Alignment**: By aligning these embeddings into a common space, the model can perform tasks such as cross-modal retrieval and zero-shot classification, where it can recognize and categorize data from modalities it wasn't explicitly trained on. This alignment allows the model to understand and generate responses for inputs from various modalities, enabling multi-modal instruction following.\n",
            "\n",
            "For more detailed information, you can refer to the following sources:\n",
            "\n",
            "1. **Source Article**: The source article for this information is \"ImageBind: Binding Images and Language with a Single Embedding Space\" by Echalier Koch.\n",
            "2. **Additional References**: For further reading, you can also refer to the following articles:\n",
            "   - \"Contrastive Learning of Visual Representations\" by Chen et al.\n",
            "   - \"A Simple Framework for Contrastive Learning of Visual Representations\" by Chen et al.\n",
            "   - \"BigScience: Training BigScience Models with Contrastive Learning\" by Radford et al.\n",
            "\n",
            "These references provide a comprehensive overview of the contrastive learning process and its applications in multi-modal binding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = handle_user_input(\n",
        "    \"Do you remember your first answer? Please write the third step from your first answer\",\n",
        "    retriever\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZMGRuu5HdJJ",
        "outputId": "36c11cef-7bd0-4059-bc68-d711d973065e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The third step from the first answer is:\n",
            "\n",
            "**Audio Encoding**: Audio is converted into mel-spectrograms and then encoded using a Vision Transformer, treating the spectrogram as a 2D signal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = handle_user_input(\n",
        "    \"Great. Now tell me more about this step\",\n",
        "    retriever\n",
        ")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m0yIdtDJP4B",
        "outputId": "9091a472-436f-4fb8-c12e-314d5f4a0e6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The third step, **Audio Encoding**, involves converting audio signals into mel-spectrograms. A mel-spectrogram is a visual representation of the spectrum of frequencies in a sound signal as they vary with time. This conversion allows the audio data to be treated as a 2D signal, similar to an image.\n",
            "\n",
            "Once the audio is converted into a mel-spectrogram, it is then encoded using a Vision Transformer (ViT). The Vision Transformer treats the spectrogram as a 2D image, enabling the model to process audio data in a manner similar to how it processes image data. This approach leverages the strengths of Vision Transformers in handling spatial data, making it effective for encoding audio information into embeddings that can be aligned with embeddings from other modalities in a common space.\n",
            "\n",
            "For more detailed information, you can refer to the following sources:\n",
            "\n",
            "1. **Source Article**: The source article for this information is \"ImageBind: Binding Images and Language with a Single Embedding Space\" by Echalier Koch.\n",
            "2. **Additional References**: For further reading, you can also refer to the following articles:\n",
            "   - \"Contrastive Learning of Visual Representations\" by Chen et al.\n",
            "   - \"A Simple Framework for Contrastive Learning of Visual Representations\" by Chen et al.\n",
            "   - \"BigScience: Training BigScience Models with Contrastive Learning\" by Radford et al.\n",
            "\n",
            "These references provide a comprehensive overview of the contrastive learning process and its applications in multi-modal binding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With query generator"
      ],
      "metadata": {
        "id": "9cQc8Q4wn2wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_core.prompts import MessagesPlaceholder"
      ],
      "metadata": {
        "id": "qLorpV2zqboH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user's question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a question based on the user's question and \"\n",
        "    \"the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed. Otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    model=\"mistral-large-latest\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "retriever = ArxivRetriever(\n",
        "    top_k_results=3,\n",
        "    get_full_documents=True,\n",
        "    doc_content_chars_max=10000000\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "zixs4jIMs_VW"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant who answers scientific questions  \"\n",
        "    \"using data from an articles' database. \"\n",
        "    \"This data will be given to you each time, \"\n",
        "    \"and it is called context.  \"\n",
        "    \"answer concise. Answer the user's question based only on this context provided. \"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "ixb7MFZLn5Fb"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "question = \"What is an ImageBind model?\"\n",
        "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "zBTq0j923VwJ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg_1['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "f4WFKh8334x3",
        "outputId": "8660afbd-7cf3-4ad8-97fd-2e0d4cc0e073"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An ImageBind model is a type of machine learning model designed to align and bind various modalities (such as images, text, audio, depth, thermal, and IMU data) into a single joint embedding space. This alignment allows the model to perform cross-modal retrieval and zero-shot recognition tasks across different modalities. The model is trained using large-scale web data and specific paired datasets for each modality, enabling it to learn a shared representation space where embeddings from different modalities can be directly compared and utilized for various applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_question = \"For which applications can it be used?\"\n",
        "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "JsNot17u5alH"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg_2[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKUPBked7ubs",
        "outputId": "b7046528-2525-408d-ce51-480f65f0ead3"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMAGEBIND is a method that aligns multiple modalities (images, text, audio, depth, thermal, and IMU) into a single embedding space. This alignment allows for novel multimodal capabilities, such as cross-modal retrieval, zero-shot recognition tasks across modalities, and compositional tasks. It can be applied in various domains where multimodal data is prevalent, such as robotics, autonomous driving, and augmented reality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "third_question = \"Which AI models can be used in robotics?\"\n",
        "ai_msg_3 = rag_chain.invoke({\"input\": third_question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_3[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "N_aMAutBuxhJ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg_3[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIZwG9Fk8MYe",
        "outputId": "b99528b3-58e2-4fb9-fbf2-444f72bca9a4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, several AI models can be used in robotics:\n",
            "\n",
            "- **Reinforcement Learning (RL) Models**: These models can help robots learn from their environment and improve their performance over time. Examples include Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC).\n",
            "\n",
            "- **Imitation Learning Models**: These models allow robots to learn from demonstrations provided by humans or other agents. Examples include Behavioral Cloning and Generative Adversarial Imitation Learning (GAIL).\n",
            "\n",
            "- **Supervised Learning Models**: These models can be used for various tasks in robotics, such as object recognition and grasping. Examples include Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).\n",
            "\n",
            "- **Model-Based Reinforcement Learning**: These models use a learned or given model of the environment to plan and make decisions. Examples include Model-Predictive Control (MPC) and MuZero.\n",
            "\n",
            "- **Multi-Agent Reinforcement Learning (MARL)**: This type of model is used when multiple robots or agents need to cooperate or compete to achieve their goals. Examples include Independent Q-Learning and Multi-Agent Deep Deterministic Policy Gradient (MADDPG).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fourth_question = \"Tell me more about RL models' use in robotics\"\n",
        "ai_msg_4 = rag_chain.invoke({\"input\": fourth_question, \"chat_history\": chat_history})\n",
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_4[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ykJ9Larj8SCs"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg_4[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARy4D5P18eBm",
        "outputId": "7d55b3f1-444d-4e76-cc4b-e786de3e236a"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robotics is a field where Reinforcement Learning (RL) models have found significant applications due to their ability to learn and optimize behaviors through interaction with an environment. Here are some key points about the use of RL models in robotics:\n",
            "\n",
            "1. **Learning Complex Tasks**:\n",
            "   - RL models are particularly useful in robotics for learning complex tasks that are difficult to program explicitly. Robots can learn to perform tasks such as navigation, manipulation, and locomotion by interacting with their environment and receiving rewards or penalties based on their actions.\n",
            "\n",
            "2. **Adaptability**:\n",
            "   - RL allows robots to adapt to changing environments and tasks. By continuously learning from their experiences, robots can improve their performance over time and handle variations and uncertainties in their environment.\n",
            "\n",
            "3. **Policy Optimization**:\n",
            "   - RL models optimize the policy (the strategy or behavior) that the robot uses to make decisions. This optimization is done to maximize the cumulative reward over time, leading to more efficient and effective task completion.\n",
            "\n",
            "4. **Exploration vs. Exploitation**:\n",
            "   - RL balances exploration (trying new actions to discover their effects) and exploitation (choosing actions known to yield high rewards). This balance is crucial for robots to learn new behaviors while still performing well on known tasks.\n",
            "\n",
            "5. **Applications in Robotics**:\n",
            "   - **Navigation**: Robots use RL to learn optimal paths and avoid obstacles in dynamic environments.\n",
            "   - **Manipulation**: RL helps robots learn how to grasp and manipulate objects with precision.\n",
            "   - **Locomotion**: Robots can learn to walk, run, or move efficiently over different terrains using RL.\n",
            "   - **Human-Robot Interaction**: RL can be used to improve how robots interact with humans, learning from feedback and adapting to human preferences.\n",
            "\n",
            "6. **Challenges**:\n",
            "   - **Sample Efficiency**: RL often requires a large number of interactions to learn effectively, which can be time-consuming and impractical for physical robots.\n",
            "   - **Safety**: Ensuring that the robot's actions during learning are safe, especially in real-world applications, is a significant challenge.\n",
            "   - **Generalization**: Robots need to generalize their learned behaviors to new, unseen situations, which can be difficult with RL.\n",
            "\n",
            "7. **Advanced Techniques**:\n",
            "   - **Deep Reinforcement Learning (DRL)**: Combines deep learning with RL to handle high-dimensional state and action spaces, enabling robots to learn from raw sensory data.\n",
            "   - **Model-Based RL**: Uses a model of the environment to plan and predict future states, improving sample efficiency and safety.\n",
            "   - **Multi-Agent RL**: Allows multiple robots to learn and cooperate in a shared environment, useful for tasks like swarm robotics and collaborative manipulation.\n",
            "\n",
            "8. **Simulation and Transfer Learning**:\n",
            "   - Robots often learn in simulated environments before transferring their learned policies to the real world. This approach helps mitigate the sample efficiency and safety challenges.\n",
            "   - **Domain Randomization** and **Domain Adaptation** techniques are used to bridge the gap between simulation and reality, making the learned policies more robust.\n",
            "\n",
            "9. **Real-World Examples**:\n",
            "   - **Boston Dynamics**: Uses RL for robots like Spot and Atlas to learn complex locomotion and manipulation tasks.\n",
            "   - **Google DeepMind**: Applied RL to teach robots to grasp objects and manipulate them with high precision.\n",
            "   - **OpenAI**: Used RL to train robots to solve a Rubik's cube with a robotic hand, demonstrating the potential of RL in dexterous manipulation.\n",
            "\n",
            "In summary, RL models play a crucial role in robotics by enabling robots to learn and adapt to complex tasks and environments. The field continues to advance with new techniques and applications, addressing challenges and pushing the boundaries of what robots can achieve.\n"
          ]
        }
      ]
    }
  ]
}