{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Evaluation",
   "id": "d2a5296b8e7c331d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:42:05.752967Z",
     "start_time": "2024-12-23T18:42:05.721714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')"
   ],
   "id": "3e52cadf9c06cd2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-23T18:49:45.166265Z",
     "start_time": "2024-12-23T18:49:45.150670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from rag.rag_pipeline import RAGPipeline, GPTunnelLLM\n",
    "from langchain_community.retrievers import ArxivRetriever\n",
    "from langsmith import Client, evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "from langchain import hub\n",
    "from langchain_mistralai import ChatMistralAI"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:42:14.181363Z",
     "start_time": "2024-12-23T18:42:10.449078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = ArxivRetriever(\n",
    "        top_k_results=3,\n",
    "        get_full_documents=False,  # gives errors with MuPDF when True\n",
    "        doc_content_chars_max=10000000000\n",
    "    )\n",
    "\n",
    "gptunell_key = os.environ.get('GPTUNNEL_API_KEY')\n",
    "gptunnel_llm = GPTunnelLLM(api_key=gptunell_key)\n",
    "\n",
    "assistant = RAGPipeline(llm=gptunnel_llm, retriever=retriever)\n",
    "\n",
    "# Example query\n",
    "question = \"How does ImageBind model bind multiple modalities into a single embedding space? Tell me in detail.\"\n",
    "response = assistant.handle_user_input(question)\n",
    "print(response)"
   ],
   "id": "7976c2b4629d0aae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageBind model binds multiple modalities into a single embedding space through a learnable bind network. This network aligns the embedding space between LLaMA and ImageBind's image encoder. The image features transformed by the bind network are then added to word tokens of all layers in LLaMA, progressively injecting visual instructions via an attention-free and zero-initialized gating mechanism. This process enables the model to exhibit superior multi-modality instruction-following capabilities.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:42:14.268719Z",
     "start_time": "2024-12-23T18:42:14.237459Z"
    }
   },
   "cell_type": "code",
   "source": "langsmith_client = Client()",
   "id": "57fbaa23200e2699",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:44:36.585920Z",
     "start_time": "2024-12-23T18:44:36.567920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_questions_df = pd.read_csv('data/evaluation_questions.csv')\n",
    "dataset_name = \"Arxiv RAG Evaluation Questions\""
   ],
   "id": "93278ff35ef933ae",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T17:42:28.036790Z",
     "start_time": "2024-12-23T17:42:28.017679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def correct_answer(root_run: Run, example: Example) -> dict:\n",
    "    score = root_run.outputs.get(\"output\") == example.outputs.get(\"answer\")\n",
    "    return {\"score\": int(score), \"key\": \"correct_answer\"}"
   ],
   "id": "b4fc8891c79212d3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T17:43:44.963413Z",
     "start_time": "2024-12-23T17:43:14.921686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = evaluate(\n",
    "    lambda inputs: assistant.handle_user_input(inputs[\"question\"]),\n",
    "    data=dataset_name,\n",
    "    evaluators=[correct_answer],\n",
    "    experiment_prefix=\"Arxiv RAG Queries\",\n",
    "    # description=\"Testing the baseline system.\",  # optional\n",
    ")"
   ],
   "id": "9a4dcfee9c114b66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Agent RAG Queries-355d19f5' at:\n",
      "https://smith.langchain.com/o/76b2dc9d-4b98-4e5e-983b-623eb76c0ac6/datasets/62c45b24-543f-4290-aeb6-cb696dd9cb06/compare?selectedSessions=b3a2cc3c-1bfd-4022-8534-24dd1d3e5a50\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c26dcd979f864a889ae3e857d36f85ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:44:19.461547Z",
     "start_time": "2024-12-23T18:44:19.445574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = assistant.handle_user_input(example[\"question\"])\n",
    "    return {\"answer\": response}\n",
    "\n",
    "# todo: implement context docs returning\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = assistant.handle_user_input(example[\"question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ],
   "id": "66a62cff128184d2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Response vs reference answer",
   "id": "23b45e2c370e60ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:53:31.277407Z",
     "start_time": "2024-12-23T18:53:30.313772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    reference = example.outputs[\"ground_truth\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "    \n",
    "    time.sleep(3) # isn't clear if this helps\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ],
   "id": "47677cc7484715ea",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:42:27.571600Z",
     "start_time": "2024-12-23T18:42:27.546601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )"
   ],
   "id": "f1d8a2dcce9770ea",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:42:49.162436Z",
     "start_time": "2024-12-23T18:42:48.609081Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke('who created you?')",
   "id": "beecbb374ad0d192",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I was created by Mistral AI.', response_metadata={'token_usage': {'prompt_tokens': 7, 'total_tokens': 15, 'completion_tokens': 8}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-5de10ec5-3165-4559-96bc-11b8b4a33a13-0', usage_metadata={'input_tokens': 7, 'output_tokens': 8, 'total_tokens': 15})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:54:15.395635Z",
     "start_time": "2024-12-23T18:53:35.055894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-answer-v-reference\",\n",
    ")"
   ],
   "id": "91bcd266a9baaaa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-v-reference-3c61724d' at:\n",
      "https://smith.langchain.com/o/76b2dc9d-4b98-4e5e-983b-623eb76c0ac6/datasets/62c45b24-543f-4290-aeb6-cb696dd9cb06/compare?selectedSessions=0cf2c847-7349-4625-b365-08df545700f5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2388f54513b4cfb825d6c3fb4368bfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 54318d7d-e66f-4e28-adf7-7ef4afbb546b: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 8c701819-99b4-4d45-bef2-81a37a82a089: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 699b87e0-6ab1-4004-96d2-a148fd30928c: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run a91d5d40-fdfc-4a77-8c26-3662b9568176: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 9b8fa4a0-b5e3-490c-991b-b54f9d36ce03: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 393b7e5d-c095-4640-818d-b1a4271a899d: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 58dcf6fa-62b4-4c3b-9129-350ed255de40: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run a4b9ba80-7895-45c9-8573-d541a63e98cf: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 9cca6778-be05-4fa8-bb6f-f1abfa123056: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 60881a05-1a58-4ac8-8e01-ca4bc61cf282: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_evaluator> on run 3d00d61b-c8ad-48b6-8f39-7ed2ba31bbe9: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-21-830c7579816c>\", line 26, in answer_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Response vs input",
   "id": "e1395f1cd3581cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:54:34.571552Z",
     "start_time": "2024-12-23T18:54:33.562068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ],
   "id": "60ebfc83972ff9f4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:55:15.313148Z",
     "start_time": "2024-12-23T18:54:38.293425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_helpfulness_evaluator],\n",
    "    experiment_prefix=\"rag-answer-helpfulness\",\n",
    ")"
   ],
   "id": "a7f3b8fcbc68693e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-helpfulness-60953126' at:\n",
      "https://smith.langchain.com/o/76b2dc9d-4b98-4e5e-983b-623eb76c0ac6/datasets/62c45b24-543f-4290-aeb6-cb696dd9cb06/compare?selectedSessions=9eeef7bd-3168-410e-b45d-32ed240c68a7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87d07448cbec4aa1b9fd9670a12df592"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run d3a1cadd-d07a-4168-bea6-dd2e39b71ac8: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 1ffb623b-8796-4657-bbae-e2b887961830: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 7c3e27b9-af08-4a23-9c1d-04db2577d261: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 80992d7c-d255-474e-b1e7-5c349ae4ed12: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 15e11c45-93c4-473f-8146-aa8eacf9f556: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run dcb30ce0-1e61-47ae-8f77-28e9e6bea18b: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run d1b7d736-f10b-40ad-b985-0cf02c2cf7b5: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run c25ae0dc-4774-4c7a-923f-9440230583fe: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run b258bd5a-d092-4890-baec-261a1039cd22: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n",
      "Error running evaluator <DynamicRunEvaluator answer_helpfulness_evaluator> on run 83e3ae7a-6106-4850-87f4-c04722ec8633: HTTPStatusError('Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-23-f5956559c19e>\", line 27, in answer_helpfulness_evaluator\n",
      "    score = answer_grader.invoke({\"question\": input_question,\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 531, in _generate\n",
      "    response = self.completion_with_retry(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 453, in completion_with_retry\n",
      "    rtn = _completion_with_retry(**kwargs)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 450, in _completion_with_retry\n",
      "    _raise_on_error(response)\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langchain_mistralai\\chat_models.py\", line 168, in _raise_on_error\n",
      "    raise httpx.HTTPStatusError(\n",
      "httpx.HTTPStatusError: Error response 429 while fetching https://api.mistral.ai/v1/chat/completions: {\"message\":\"Requests rate limit exceeded\"}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Response vs retrieved docs",
   "id": "61271b50cdecf8e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:55:48.784387Z",
     "start_time": "2024-12-23T18:55:47.935435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prompt\n",
    "grade_prompt_hallucinations = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ],
   "id": "5e958202248f1ad6",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:57:23.953107Z",
     "start_time": "2024-12-23T18:55:52.906586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-answer-hallucination\",\n",
    ")"
   ],
   "id": "10dd3e4d01a1d069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-hallucination-f6b4e418' at:\n",
      "https://smith.langchain.com/o/76b2dc9d-4b98-4e5e-983b-623eb76c0ac6/datasets/62c45b24-543f-4290-aeb6-cb696dd9cb06/compare?selectedSessions=1af90b89-bfe5-4c88-afda-c938d6a327da\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e46ec298fcb407db03f583713083b73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 238b353d-0a38-48c9-ad53-36a03ae7643c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 0036a1a3-62aa-4b86-a8c3-1e43dd89f67e: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run c9374b7a-e6e7-42ae-a711-47ab8ff1a7cc: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run d226c90c-eb6c-4b98-b3d2-c4dcfa6de77a: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run c85e779e-e5d9-4ac3-9fe4-e532e962efc6: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 23206962-2821-4aa7-b7ed-1e864f95e1f3: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run e1c656f8-a31d-4e50-b8a0-0df69d9a5954: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run ef70a953-1f32-449a-bf6a-4fc633434870: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 906e6cde-66b5-4bf0-a1ec-f3720d926fde: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 54639802-0fc1-4778-999c-29b80ce1cdbc: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 8b9b8be4-e59d-4903-8c18-f7900516f860: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 3b7733ce-40a2-4e7f-99ac-3f8be808a2e1: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run b26e53e9-82cb-40e5-b1b8-a0ff10e3786f: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 9e7d276a-07f5-474c-8c85-102df982e000: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 4d26b674-663a-4ede-8fe4-873e40ce596c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 3ed5555a-8035-496d-9c54-f32faa01d8db: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 4e313ddb-64b2-4d37-a93a-e830f2b7bdd6: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 32ea7109-fb9b-4046-af89-79f501040d8c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 05b90525-aea4-4840-befa-dedb272ff58e: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: string indices must be integers\n",
      "Error running evaluator <DynamicRunEvaluator answer_hallucination_evaluator> on run 6805fdb9-641d-4386-9525-b4b701371e3c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-25-bf043159ce4b>\", line 11, in answer_hallucination_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieved docs vs input",
   "id": "6eeeaf4ecc209ed8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:57:24.750464Z",
     "start_time": "2024-12-23T18:57:23.958013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Grade prompt\n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"contexts\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatMistralAI(\n",
    "        model=\"mistral-large-latest\",\n",
    "        temperature=0,\n",
    "        max_retries=2,\n",
    "        # other params...\n",
    "    )\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ],
   "id": "97c4113920f148e9",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-23T18:57:41.470263Z",
     "start_time": "2024-12-23T18:57:24.767940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    ")"
   ],
   "id": "3c481a92817423e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-ebbd0690' at:\n",
      "https://smith.langchain.com/o/76b2dc9d-4b98-4e5e-983b-623eb76c0ac6/datasets/62c45b24-543f-4290-aeb6-cb696dd9cb06/compare?selectedSessions=29432ea3-92cc-4199-8c42-29d496087df2\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dae13f74e1d4c239686235e2465eb62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 3f44c1e8-047f-479b-aea6-0d840f233a8b: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run cf4bb9fe-6246-42cb-b574-0d4badb935bc: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run e78d9edd-3edf-4246-aba5-d9db3175b318: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run ff435b41-cf56-4139-bb0a-e7fd9cdaf8cc: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 1368bea6-d866-4c6d-b4a3-43d27c9ab8aa: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 80523075-f1bd-4a15-a4d4-e5439a3dbaac: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 3bf39160-4d16-4eb7-a9c7-1b9b85c3ac02: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run c791bd5a-3c4d-44cd-a9f5-443eadc6ccb7: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 7ae5b607-0ebf-4eb4-aeff-9b2773993059: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 2e4e3f3c-8b03-47f6-8c1c-44636546394a: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 988849c7-95e5-4ee0-b2b3-0ab33a7dee60: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run fc70fa04-a314-4b3d-b5d8-20168d92dc5f: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run d21f7636-6140-48e7-8226-98065cc410b5: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run dd016fd7-3986-484e-8ee1-e0f014985ec7: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 0832104d-4065-4cd2-ba98-76efd01438e7: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run c357eb1e-046d-4bab-9d0e-39d7b138988c: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run c3968c11-21b7-4059-b8ab-5fc90aa659af: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run c3b65703-f286-455c-a973-40c96abd80a2: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 9d2a94a9-dc87-4c08-8a60-82c48bdbc4a3: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n",
      "Error running target function: 'choices'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run bf09c508-3130-4e03-b41a-ae8510faf885: KeyError('contexts')\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"C:\\Program Files\\Python38\\lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "  File \"<ipython-input-27-15f1a8fa1611>\", line 11, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"contexts\"]\n",
      "KeyError: 'contexts'\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "108c66e0522b2acd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
