Question,Answer
What is the primary difference between supervised and unsupervised learning?,"Supervised learning involves labeled data, while unsupervised learning works with unlabeled data to identify hidden structures."
"What are Transformers, and why are they important in NLP?","Transformers are a neural network architecture based on self-attention mechanisms, critical for understanding context in text and forming the backbone of models like GPT and BERT."
How do reinforcement learning algorithms differ from other machine learning methods?,"Reinforcement learning trains agents to make sequences of decisions by maximizing cumulative rewards from an environment, unlike supervised or unsupervised learning."
Explain the role of attention mechanisms in deep learning.,"Attention mechanisms allow models to focus on relevant parts of input data, improving performance in tasks like translation, summarization, and question answering."
What is the purpose of a variational autoencoder (VAE)?,VAEs are generative models used for learning latent representations of data and generating new samples from these representations.
What challenges do researchers face in training large-scale AI models?,"Common challenges include high computational costs, the need for vast labeled datasets, model interpretability, and energy efficiency."
What advancements in AI have been made possible by Graph Neural Networks (GNNs)?,"GNNs enable modeling of relationships and interactions in graph-structured data, powering advancements in social network analysis, recommendation systems, and molecular biology."
How does transfer learning improve the efficiency of training AI models?,"Transfer learning reuses knowledge from pre-trained models to solve new tasks with less data and computation, enhancing efficiency and accuracy."
What is the significance of the 'lottery ticket hypothesis' in neural networks?,"It suggests that large networks contain smaller, trainable subnetworks (winning tickets) capable of achieving similar performance, reducing computational overhead."
How do adversarial examples expose vulnerabilities in AI systems?,"Adversarial examples are specially crafted inputs that cause models to make errors, highlighting weaknesses in robustness and security."
What are the key differences between BERT and GPT architectures?,"BERT is bidirectional and focuses on understanding context in both directions, while GPT is unidirectional and optimized for generating text."
How does layer normalization improve training stability in deep networks?,"Layer normalization normalizes inputs across features within a layer, reducing internal covariate shift and improving training speed and stability."
What is the impact of dropout regularization in preventing overfitting?,"Dropout prevents overfitting by randomly deactivating neurons during training, forcing the network to learn robust and distributed representations."
How does the use of positional encodings enable Transformers to handle sequential data?,"Positional encodings inject information about the order of input tokens, allowing Transformers to capture sequence structure without recurrence."
What are the limitations of using ReLU activation functions in deep networks?,ReLU can suffer from 'dead neurons' where certain neurons stop updating weights due to outputs stuck at zero for all inputs.
What strategies can be used to mitigate mode collapse in GANs?,"Techniques include using Wasserstein loss, spectral normalization, or balancing generator-discriminator dynamics."
What is the role of batch size in training neural networks?,"Batch size affects convergence rate, training stability, and memory usage. Small batches provide noise for generalization; large batches improve gradient estimates."
How do capsule networks differ from traditional CNNs?,"Capsule networks retain spatial hierarchies by encoding part-whole relationships, unlike traditional CNNs that lose this information due to pooling."
What is the purpose of using beam search in sequence generation tasks?,"Beam search explores multiple candidate sequences simultaneously, balancing between exploration and exploitation for better sequence predictions."
How does knowledge distillation improve the performance of smaller models?,Knowledge distillation transfers learned representations from a larger model (teacher) to a smaller model (student) to improve its accuracy and efficiency.
